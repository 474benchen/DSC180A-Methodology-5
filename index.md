# Hear ye, hear ye! 
## <center>Benjamin Chen (bhc001@ucsd.edu)</center>
### <center>A10 - ETHICAL CONSIDERATIONS IN USING AI</center>
### <center>Emily Ramond / Rasmus Nielsen</center>
<!---
I added ^ that so that it's included in index.md; makes my website look ugly though :(
--->

Hi Praveen, before I answer the MA questions, I thought it imperative for you to see this cat:
<img src='/imgs/cat.png'>


1. **What is the most interesting topic covered in your domain this quarter?**

    Fairness Metrics - I think that this is a super niche topic that I've never encountered besides when I've explicitly searched for it. I think metrics like balanced error rate 
    or disparate impact are pretty intuitive, but there's so many other ones that we've never heard of until now and it's pretty interesting to learn about their use cases
    and differences/implications. Not only covering them conceptually, but seeing them in practice on actual models is really interesting. It's an aspect of machine learning
    that we're never really taught.

2. **Describe a potential investigation you would like to pursue for your Quarter 2 Project.**

    This would need more fleshing out, but I'm pretty interested in investigating whether AI predicting plagiarism/cheating are biased in anyway. This is a relatively difficult
    topic to get ahold of data for, but it would be something along the lines of a dataset that has an individual's metadata (age, race, school, etc.), their work (essay, code, etc.)
    and a binary check of whether it was plagirized or not (could be a "risk" score as well). We could then use the fairness metrics that we learned this quarter or even
    a statistical test to see if specific groups were unfairly impacted. Pretty interesting in my opinion, but hard to get off the ground in terms of data. Especially when
    sensitive data like that is involved (and on a niche topic).

3. **What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?**

    I don't like that we have pure writers on the team. I think, especially with chatGPT, the writing role is an excuse to do marginally less work than the coders. While I can see
    writing being a major part of someone's role, I think they should still be responsible for coding. Granted, my team does consist of 5 people, which makes it extremely difficult
    to have everyone code (too many cooks spoil the broth, and boy am I cooking), but I still think the writers should do more to contribute a "fair" share. 

    I also don't like that teams are basically randomly assigned (in the sense that you don't know anyone). Maybe this is a symptom of my year since we had COVID, but I feel
    like I didn't get to gauge the abilities of 70% of the people in my capstone properly, so even though I basically got to pick my team, I still didn't know what I would be
    getting for teammates. I wish there were assignments earlier that let us see any information about our teammates, or even having us submit our resumes as a fair
    screening to each other. I get that this simulates real life and you don't get to always work with competent people, but I feel like it's extremely frustrating
    to be grouped with people who are questionable at best.

4. **What other techniques would you be interested in using in your project?**

    I never thought I'd say this, but I actually wish more statistics was involved in my capstone right now. I think including hypothesis testing in a fairness
    framework would be pretty interesting. Of course, it IS just hypothesis testing, but seeing how bias can be proven statistically is as interesting
    as the fairness metrics that we're introduced to in my opinion.

    I also wish we had to use DSMLP, but it's hard for my capstone to justify it as our dataset is small and it's easier/more familiar for us to work through
    local jupyter and github.

    This really sounds like I'm just complaining... 